{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc88088",
   "metadata": {
    "papermill": {
     "duration": 0.014751,
     "end_time": "2023-08-15T00:21:27.418951",
     "exception": false,
     "start_time": "2023-08-15T00:21:27.404200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NeurIPS 2023 Machine Unlearning Challenge\n",
    "Heavily based off of the code presented here: https://github.com/unlearning-challenge/starting-kit/blob/main/unlearning-CIFAR10.ipynb\n",
    "\n",
    "Relevant Links:\n",
    "Fast Yet Effective Unlearning: https://arxiv.org/pdf/2111.08947.pdf and [code repo](https://github.com/vikram2000b/Fast-Machine-Unlearning/blob/main/Machine%20Unlearning.ipynb)\n",
    "\n",
    "LEACE: https://arxiv.org/pdf/2306.03819.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f10162d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-15T00:21:27.450011Z",
     "iopub.status.busy": "2023-08-15T00:21:27.449598Z",
     "iopub.status.idle": "2023-08-15T00:21:27.477085Z",
     "shell.execute_reply": "2023-08-15T00:21:27.476219Z"
    },
    "papermill": {
     "duration": 0.045951,
     "end_time": "2023-08-15T00:21:27.479901",
     "exception": false,
     "start_time": "2023-08-15T00:21:27.433950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc98fd55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:21:27.512813Z",
     "iopub.status.busy": "2023-08-15T00:21:27.512415Z",
     "iopub.status.idle": "2023-08-15T00:21:40.154648Z",
     "shell.execute_reply": "2023-08-15T00:21:40.153664Z"
    },
    "papermill": {
     "duration": 12.662556,
     "end_time": "2023-08-15T00:21:40.157373",
     "exception": false,
     "start_time": "2023-08-15T00:21:27.494817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: CPU\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:02<00:00, 78352764.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, model_selection\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device:\", DEVICE.upper())\n",
    "\n",
    "# manual random seed is used for dataset partitioning\n",
    "# to ensure reproducible results across runs\n",
    "RNG = torch.Generator().manual_seed(42)\n",
    "\n",
    "# download and pre-process CIFAR10\n",
    "normalize = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=normalize\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# we split held out data into test and validation set\n",
    "held_out = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=normalize\n",
    ")\n",
    "test_set, val_set = torch.utils.data.random_split(held_out, [0.5, 0.5], generator=RNG)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# download the forget and retain index split\n",
    "local_path = \"forget_idx.npy\"\n",
    "if not os.path.exists(local_path):\n",
    "    response = requests.get(\n",
    "        \"https://unlearning-challenge.s3.eu-west-1.amazonaws.com/cifar10/\" + local_path\n",
    "    )\n",
    "    open(local_path, \"wb\").write(response.content)\n",
    "forget_idx = np.load(local_path)\n",
    "\n",
    "# construct indices of retain from those of the forget set\n",
    "forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
    "forget_mask[forget_idx] = True\n",
    "retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
    "\n",
    "# split train set into a forget and a retain set\n",
    "forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
    "retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
    "\n",
    "forget_loader = torch.utils.data.DataLoader(\n",
    "    forget_set, batch_size=128, shuffle=True, num_workers=2\n",
    ")\n",
    "retain_loader = torch.utils.data.DataLoader(\n",
    "    retain_set, batch_size=128, shuffle=True, num_workers=2, generator=RNG\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3212b939",
   "metadata": {
    "papermill": {
     "duration": 0.017186,
     "end_time": "2023-08-15T00:21:40.191700",
     "exception": false,
     "start_time": "2023-08-15T00:21:40.174514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since we can't download the aws weights, I've manually uploaded them to this Kaggle notebook as `unlearning-cifar10-resnet18-weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349ee44a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:21:40.230421Z",
     "iopub.status.busy": "2023-08-15T00:21:40.229277Z",
     "iopub.status.idle": "2023-08-15T00:21:43.341154Z",
     "shell.execute_reply": "2023-08-15T00:21:43.339535Z"
    },
    "papermill": {
     "duration": 3.134705,
     "end_time": "2023-08-15T00:21:43.344032",
     "exception": false,
     "start_time": "2023-08-15T00:21:40.209327",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download pre-trained weights\n",
    "local_path = \"weights_resnet18_cifar10.pth\"\n",
    "if not os.path.exists(local_path):\n",
    "    response = requests.get(\n",
    "        \"https://unlearning-challenge.s3.eu-west-1.amazonaws.com/weights_resnet18_cifar10.pth\"\n",
    "    )\n",
    "    open(local_path, \"wb\").write(response.content)\n",
    "\n",
    "weights_pretrained = torch.load(local_path, map_location=DEVICE)\n",
    "\n",
    "# load model with pre-trained weights\n",
    "model = resnet18(weights=None, num_classes=10)\n",
    "model.load_state_dict(weights_pretrained)\n",
    "model.to(DEVICE)\n",
    "model.eval();\n",
    "\n",
    "# weights_pretrained = torch.load('/kaggle/input/unlearning-cifar10-resnet18-weights/unlearning-cifar10-resnet18-weights/weights_resnet18_cifar10.pth', map_location=DEVICE)\n",
    "# model = resnet18(weights=None, num_classes=10)\n",
    "# model.load_state_dict(weights_pretrained)\n",
    "# model.to(DEVICE)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889a5d4",
   "metadata": {
    "papermill": {
     "duration": 0.016699,
     "end_time": "2023-08-15T00:21:43.377956",
     "exception": false,
     "start_time": "2023-08-15T00:21:43.361257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Look at sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec636dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:21:43.415024Z",
     "iopub.status.busy": "2023-08-15T00:21:43.414340Z",
     "iopub.status.idle": "2023-08-15T00:21:43.424265Z",
     "shell.execute_reply": "2023-08-15T00:21:43.422690Z"
    },
    "papermill": {
     "duration": 0.031891,
     "end_time": "2023-08-15T00:21:43.427341",
     "exception": false,
     "start_time": "2023-08-15T00:21:43.395450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# a temporary data loader without normalization, just to show the images\n",
    "tmp_dl = DataLoader(\n",
    "    torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transforms.ToTensor()\n",
    "    ),\n",
    "    batch_size=16 * 5,\n",
    "    shuffle=False,\n",
    ")\n",
    "images, labels = next(iter(tmp_dl))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title(\"Sample images from CIFAR10 dataset\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1cac36",
   "metadata": {
    "papermill": {
     "duration": 0.018837,
     "end_time": "2023-08-15T00:21:43.463374",
     "exception": false,
     "start_time": "2023-08-15T00:21:43.444537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Accuracy Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cefa336f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:21:43.501711Z",
     "iopub.status.busy": "2023-08-15T00:21:43.501302Z",
     "iopub.status.idle": "2023-08-15T00:21:43.509131Z",
     "shell.execute_reply": "2023-08-15T00:21:43.507831Z"
    },
    "papermill": {
     "duration": 0.029873,
     "end_time": "2023-08-15T00:21:43.512001",
     "exception": false,
     "start_time": "2023-08-15T00:21:43.482128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(net, loader):\n",
    "    \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in tqdm(loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1f24a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:21:43.548857Z",
     "iopub.status.busy": "2023-08-15T00:21:43.547859Z",
     "iopub.status.idle": "2023-08-15T00:21:43.556912Z",
     "shell.execute_reply": "2023-08-15T00:21:43.555864Z"
    },
    "papermill": {
     "duration": 0.030007,
     "end_time": "2023-08-15T00:21:43.559549",
     "exception": false,
     "start_time": "2023-08-15T00:21:43.529542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "print(f\"Train set accuracy: {100.0 * accuracy(model, train_loader):0.1f}%\")\n",
    "print(f\"Test set accuracy: {100.0 * accuracy(model, test_loader):0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ff451",
   "metadata": {
    "papermill": {
     "duration": 0.017102,
     "end_time": "2023-08-15T00:21:43.593878",
     "exception": false,
     "start_time": "2023-08-15T00:21:43.576776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Unlearning Algorithm\n",
    "In this section we develop the unlearning algorithm.\n",
    "\n",
    "In the previous section we created a split of the original training set into a retain set and a forget set. Typically, the retain set is much larger than the forget set. Here, we produce a split that is 10% forget set, and 90% retain set.\n",
    "\n",
    "The goal of an unlearning algorithm is to produce a model that approximates as much as possible the model trained solely on the retain set.\n",
    "\n",
    "Below is a simple unlearning algorithm provided for illustration purposes. We call this algorithm unlearning by fine-tuning. It starts from the pre-trained model and optimizes it for a few epochs on the retain set. This is a very simple unlearning algorithm, but it is not very computationally efficient, and we don't expect it to work very well for most metrics.\n",
    "\n",
    "To make a new entry in the competition, participants will submit an unlearning function with the same API as the one below. Note that the unlearning function takes as input a pre-trained model, a retain set, a forget set and an evaluation set (even though the fine-tuning algorithm below only uses the retain set and ignores the other datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4522ec1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:21:43.629671Z",
     "iopub.status.busy": "2023-08-15T00:21:43.629198Z",
     "iopub.status.idle": "2023-08-15T00:21:43.641371Z",
     "shell.execute_reply": "2023-08-15T00:21:43.639994Z"
    },
    "papermill": {
     "duration": 0.033392,
     "end_time": "2023-08-15T00:21:43.644246",
     "exception": false,
     "start_time": "2023-08-15T00:21:43.610854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unlearning(net, retain, forget, validation):\n",
    "    \"\"\"Unlearning by fine-tuning.\n",
    "\n",
    "    Fine-tuning is a very simple algorithm that trains using only\n",
    "    the retain set.\n",
    "\n",
    "    Args:\n",
    "      net : nn.Module.\n",
    "        pre-trained model to use as base of unlearning.\n",
    "      retain : torch.utils.data.DataLoader.\n",
    "        Dataset loader for access to the retain set. This is the subset\n",
    "        of the training set that we don't want to forget.\n",
    "      forget : torch.utils.data.DataLoader.\n",
    "        Dataset loader for access to the forget set. This is the subset\n",
    "        of the training set that we want to forget. This method doesn't\n",
    "        make use of the forget set.\n",
    "      validation : torch.utils.data.DataLoader.\n",
    "        Dataset loader for access to the validation set. This method doesn't\n",
    "        make use of the validation set.\n",
    "    Returns:\n",
    "      net : updated model\n",
    "    \"\"\"\n",
    "    epochs = 5\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    net.train()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        print(f'epoch {_}')\n",
    "        for inputs, targets in tqdm(retain):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    net.eval()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf6d148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:21:43.680664Z",
     "iopub.status.busy": "2023-08-15T00:21:43.680271Z",
     "iopub.status.idle": "2023-08-15T00:21:43.929428Z",
     "shell.execute_reply": "2023-08-15T00:21:43.928273Z"
    },
    "papermill": {
     "duration": 0.270558,
     "end_time": "2023-08-15T00:21:43.932289",
     "exception": false,
     "start_time": "2023-08-15T00:21:43.661731",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model = resnet18(weights=None, num_classes=10)\n",
    "ft_model.load_state_dict(weights_pretrained)\n",
    "ft_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2db50754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:21:43.969731Z",
     "iopub.status.busy": "2023-08-15T00:21:43.968513Z",
     "iopub.status.idle": "2023-08-15T00:21:43.977113Z",
     "shell.execute_reply": "2023-08-15T00:21:43.976289Z"
    },
    "papermill": {
     "duration": 0.029683,
     "end_time": "2023-08-15T00:21:43.979589",
     "exception": false,
     "start_time": "2023-08-15T00:21:43.949906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Execute the unlearing routine. This might take a few minutes.\n",
    "# If run on colab, be sure to be running it on  an instance with GPUs\n",
    "ft_model = unlearning(ft_model, retain_loader, forget_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0e56e98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:21:44.020322Z",
     "iopub.status.busy": "2023-08-15T00:21:44.019602Z",
     "iopub.status.idle": "2023-08-15T00:21:44.027086Z",
     "shell.execute_reply": "2023-08-15T00:21:44.026289Z"
    },
    "papermill": {
     "duration": 0.032157,
     "end_time": "2023-08-15T00:21:44.029595",
     "exception": false,
     "start_time": "2023-08-15T00:21:43.997438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "print(f\"Retain set accuracy: {100.0 * accuracy(ft_model, retain_loader):0.1f}%\")\n",
    "print(f\"Test set accuracy: {100.0 * accuracy(ft_model, test_loader):0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "750c37ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:21:44.067569Z",
     "iopub.status.busy": "2023-08-15T00:21:44.066866Z",
     "iopub.status.idle": "2023-08-15T00:21:44.074000Z",
     "shell.execute_reply": "2023-08-15T00:21:44.072690Z"
    },
    "papermill": {
     "duration": 0.029526,
     "end_time": "2023-08-15T00:21:44.076798",
     "exception": false,
     "start_time": "2023-08-15T00:21:44.047272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_losses(net, loader):\n",
    "    \"\"\"Auxiliary function to compute per-sample losses\"\"\"\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    all_losses = []\n",
    "\n",
    "    for inputs, targets in tqdm(loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        logits = net(inputs)\n",
    "        losses = criterion(logits, targets).numpy(force=True)\n",
    "        for l in losses:\n",
    "            all_losses.append(l)\n",
    "\n",
    "    return np.array(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d35b507c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:21:44.114448Z",
     "iopub.status.busy": "2023-08-15T00:21:44.113319Z",
     "iopub.status.idle": "2023-08-15T00:23:01.073987Z",
     "shell.execute_reply": "2023-08-15T00:23:01.072546Z"
    },
    "papermill": {
     "duration": 76.982183,
     "end_time": "2023-08-15T00:23:01.076646",
     "exception": false,
     "start_time": "2023-08-15T00:21:44.094463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:09<00:00,  5.59it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  5.76it/s]\n"
     ]
    }
   ],
   "source": [
    "train_losses = compute_losses(model, train_loader)\n",
    "test_losses = compute_losses(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4063fcfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:23:01.192603Z",
     "iopub.status.busy": "2023-08-15T00:23:01.191823Z",
     "iopub.status.idle": "2023-08-15T00:23:01.202396Z",
     "shell.execute_reply": "2023-08-15T00:23:01.201119Z"
    },
    "papermill": {
     "duration": 0.071548,
     "end_time": "2023-08-15T00:23:01.205090",
     "exception": false,
     "start_time": "2023-08-15T00:23:01.133542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# plot losses on train and test set\n",
    "plt.title(\"Losses on train and test set (pre-trained model)\")\n",
    "plt.hist(test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "plt.hist(train_losses, density=True, alpha=0.5, bins=50, label=\"Train set\")\n",
    "plt.xlabel(\"Loss\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.xlim((0, np.max(test_losses)))\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(frameon=False, fontsize=14)\n",
    "ax = plt.gca()\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2c2c09e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:23:01.318516Z",
     "iopub.status.busy": "2023-08-15T00:23:01.317835Z",
     "iopub.status.idle": "2023-08-15T00:23:01.325854Z",
     "shell.execute_reply": "2023-08-15T00:23:01.324863Z"
    },
    "papermill": {
     "duration": 0.066795,
     "end_time": "2023-08-15T00:23:01.327952",
     "exception": false,
     "start_time": "2023-08-15T00:23:01.261157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_mia(sample_loss, members, n_splits=10, random_state=0):\n",
    "    \"\"\"Computes cross-validation score of a membership inference attack.\n",
    "\n",
    "    Args:\n",
    "      sample_loss : array_like of shape (n,).\n",
    "        objective function evaluated on n samples.\n",
    "      members : array_like of shape (n,),\n",
    "        whether a sample was used for training.\n",
    "      n_splits: int\n",
    "        number of splits to use in the cross-validation.\n",
    "    Returns:\n",
    "      scores : array_like of size (n_splits,)\n",
    "    \"\"\"\n",
    "\n",
    "    unique_members = np.unique(members)\n",
    "    if not np.all(unique_members == np.array([0, 1])):\n",
    "        raise ValueError(\"members should only have 0 and 1s\")\n",
    "\n",
    "    attack_model = linear_model.LogisticRegression()\n",
    "    cv = model_selection.StratifiedShuffleSplit(\n",
    "        n_splits=n_splits, random_state=random_state\n",
    "    )\n",
    "    return model_selection.cross_val_score(\n",
    "        attack_model, sample_loss, members, cv=cv, scoring=\"accuracy\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88bab70d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:23:01.440782Z",
     "iopub.status.busy": "2023-08-15T00:23:01.440047Z",
     "iopub.status.idle": "2023-08-15T00:23:01.447867Z",
     "shell.execute_reply": "2023-08-15T00:23:01.446952Z"
    },
    "papermill": {
     "duration": 0.067016,
     "end_time": "2023-08-15T00:23:01.450296",
     "exception": false,
     "start_time": "2023-08-15T00:23:01.383280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "forget_losses = compute_losses(model, forget_loader)\n",
    "\n",
    "# Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
    "np.random.shuffle(forget_losses)\n",
    "forget_losses = forget_losses[: len(test_losses)]\n",
    "\n",
    "samples_mia = np.concatenate((test_losses, forget_losses)).reshape((-1, 1))\n",
    "labels_mia = [0] * len(test_losses) + [1] * len(forget_losses)\n",
    "\n",
    "mia_scores = simple_mia(samples_mia, labels_mia)\n",
    "\n",
    "print(\n",
    "    f\"The MIA has an accuracy of {mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cbe94bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:23:01.564092Z",
     "iopub.status.busy": "2023-08-15T00:23:01.563658Z",
     "iopub.status.idle": "2023-08-15T00:23:01.572389Z",
     "shell.execute_reply": "2023-08-15T00:23:01.571561Z"
    },
    "papermill": {
     "duration": 0.06859,
     "end_time": "2023-08-15T00:23:01.574610",
     "exception": false,
     "start_time": "2023-08-15T00:23:01.506020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "ft_forget_losses = compute_losses(ft_model, forget_loader)\n",
    "ft_test_losses = compute_losses(ft_model, test_loader)\n",
    "\n",
    "# make sure we have a balanced dataset for the MIA\n",
    "assert len(ft_test_losses) == len(ft_forget_losses)\n",
    "\n",
    "ft_samples_mia = np.concatenate((ft_test_losses, ft_forget_losses)).reshape((-1, 1))\n",
    "labels_mia = [0] * len(ft_test_losses) + [1] * len(ft_forget_losses)\n",
    "\n",
    "\n",
    "\n",
    "ft_mia_scores = simple_mia(ft_samples_mia, labels_mia)\n",
    "\n",
    "print(\n",
    "    f\"The MIA has an accuracy of {ft_mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b235351e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:23:01.689333Z",
     "iopub.status.busy": "2023-08-15T00:23:01.688598Z",
     "iopub.status.idle": "2023-08-15T00:23:01.697024Z",
     "shell.execute_reply": "2023-08-15T00:23:01.695864Z"
    },
    "papermill": {
     "duration": 0.068567,
     "end_time": "2023-08-15T00:23:01.699758",
     "exception": false,
     "start_time": "2023-08-15T00:23:01.631191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.set_title(f\"Pre-trained model.\\nAttack accuracy: {mia_scores.mean():0.2f}\")\n",
    "ax1.hist(test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "ax1.hist(forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "ax2.set_title(\n",
    "    f\"Unlearned by fine-tuning.\\nAttack accuracy: {ft_mia_scores.mean():0.2f}\"\n",
    ")\n",
    "ax2.hist(ft_test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "ax2.hist(ft_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "ax1.set_xlabel(\"Loss\")\n",
    "ax2.set_xlabel(\"Loss\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax1.set_xlim((0, np.max(test_losses)))\n",
    "ax2.set_xlim((0, np.max(test_losses)))\n",
    "for ax in (ax1, ax2):\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "ax1.legend(frameon=False, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d186eb",
   "metadata": {
    "papermill": {
     "duration": 0.054818,
     "end_time": "2023-08-15T00:23:01.810426",
     "exception": false,
     "start_time": "2023-08-15T00:23:01.755608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Re-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdea8297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:23:01.923555Z",
     "iopub.status.busy": "2023-08-15T00:23:01.923111Z",
     "iopub.status.idle": "2023-08-15T00:24:11.919287Z",
     "shell.execute_reply": "2023-08-15T00:24:11.917807Z"
    },
    "papermill": {
     "duration": 70.056385,
     "end_time": "2023-08-15T00:24:11.921549",
     "exception": false,
     "start_time": "2023-08-15T00:23:01.865164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:59<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain set accuracy: 99.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget set accuracy: 88.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# weights_pretrained = torch.load('/kaggle/input/unlearning-cifar10-resnet18-weights/unlearning-cifar10-resnet18-weights/retrain_weights_resnet18_cifar10.pth', map_location=DEVICE)\n",
    "\n",
    "# # load model with pre-trained weights\n",
    "# rt_model = resnet18(weights=None, num_classes=10)\n",
    "# rt_model.load_state_dict(weights_pretrained)\n",
    "# rt_model.to(DEVICE)\n",
    "# rt_model.eval()\n",
    "\n",
    "# download weights of a model trained exclusively on the retain set\n",
    "local_path = \"retrain_weights_resnet18_cifar10.pth\"\n",
    "if not os.path.exists(local_path):\n",
    "    response = requests.get(\n",
    "        \"https://unlearning-challenge.s3.eu-west-1.amazonaws.com/cifar10/\" + local_path\n",
    "    )\n",
    "    open(local_path, \"wb\").write(response.content)\n",
    "\n",
    "weights_pretrained = torch.load(local_path, map_location=DEVICE)\n",
    "\n",
    "# load model with pre-trained weights\n",
    "rt_model = resnet18(weights=None, num_classes=10)\n",
    "rt_model.load_state_dict(weights_pretrained)\n",
    "rt_model.to(DEVICE)\n",
    "rt_model.eval()\n",
    "\n",
    "# print its accuracy on retain and forget set\n",
    "print(f\"Retain set accuracy: {100.0 * accuracy(rt_model, retain_loader):0.1f}%\")\n",
    "print(f\"Forget set accuracy: {100.0 * accuracy(rt_model, forget_loader):0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19a2ee40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:24:12.103903Z",
     "iopub.status.busy": "2023-08-15T00:24:12.103443Z",
     "iopub.status.idle": "2023-08-15T00:24:12.113618Z",
     "shell.execute_reply": "2023-08-15T00:24:12.112468Z"
    },
    "papermill": {
     "duration": 0.105047,
     "end_time": "2023-08-15T00:24:12.116421",
     "exception": false,
     "start_time": "2023-08-15T00:24:12.011374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "rt_test_losses = compute_losses(rt_model, test_loader)\n",
    "rt_forget_losses = compute_losses(rt_model, forget_loader)\n",
    "\n",
    "rt_samples_mia = np.concatenate((rt_test_losses, rt_forget_losses)).reshape((-1, 1))\n",
    "labels_mia = [0] * len(rt_test_losses) + [1] * len(rt_forget_losses)\n",
    "\n",
    "rt_mia_scores = simple_mia(rt_samples_mia, labels_mia)\n",
    "\n",
    "print(\n",
    "    f\"The MIA has an accuracy of {rt_mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b9eedc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:24:12.299744Z",
     "iopub.status.busy": "2023-08-15T00:24:12.299304Z",
     "iopub.status.idle": "2023-08-15T00:24:12.308169Z",
     "shell.execute_reply": "2023-08-15T00:24:12.307001Z"
    },
    "papermill": {
     "duration": 0.102819,
     "end_time": "2023-08-15T00:24:12.310765",
     "exception": false,
     "start_time": "2023-08-15T00:24:12.207946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.set_title(f\"Re-trained model.\\nAttack accuracy: {rt_mia_scores.mean():0.2f}\")\n",
    "ax1.hist(rt_test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "ax1.hist(rt_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "ax2.set_title(\n",
    "    f\"Unlearned by fine-tuning.\\nAttack accuracy: {ft_mia_scores.mean():0.2f}\"\n",
    ")\n",
    "ax2.hist(ft_test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "ax2.hist(ft_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "ax1.set_xlabel(\"Loss\")\n",
    "ax2.set_xlabel(\"Loss\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax1.set_xlim((0, np.max(test_losses)))\n",
    "ax2.set_xlim((0, np.max(test_losses)))\n",
    "for ax in (ax1, ax2):\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "ax1.legend(frameon=False, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db0b79",
   "metadata": {
    "papermill": {
     "duration": 0.089464,
     "end_time": "2023-08-15T00:24:12.490720",
     "exception": false,
     "start_time": "2023-08-15T00:24:12.401256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Attempts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "893dec06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:24:12.673583Z",
     "iopub.status.busy": "2023-08-15T00:24:12.673162Z",
     "iopub.status.idle": "2023-08-15T00:24:12.889872Z",
     "shell.execute_reply": "2023-08-15T00:24:12.888561Z"
    },
    "papermill": {
     "duration": 0.311051,
     "end_time": "2023-08-15T00:24:12.892491",
     "exception": false,
     "start_time": "2023-08-15T00:24:12.581440",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input model load\n",
    "in_model = resnet18(weights=None, num_classes=10)\n",
    "in_model.load_state_dict(weights_pretrained)\n",
    "in_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df21c0",
   "metadata": {
    "papermill": {
     "duration": 0.089904,
     "end_time": "2023-08-15T00:24:13.074175",
     "exception": false,
     "start_time": "2023-08-15T00:24:12.984271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Attempt v0.0\n",
    "Just a test to see what happens when you multiply the loss value by -1 when training using the forget dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "391e994c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:24:13.257522Z",
     "iopub.status.busy": "2023-08-15T00:24:13.257087Z",
     "iopub.status.idle": "2023-08-15T00:24:13.273459Z",
     "shell.execute_reply": "2023-08-15T00:24:13.272102Z"
    },
    "papermill": {
     "duration": 0.111642,
     "end_time": "2023-08-15T00:24:13.275990",
     "exception": false,
     "start_time": "2023-08-15T00:24:13.164348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def repair_model(net, retain, itter):\n",
    "    \"\"\"Repairs the given model using a couple itterations\n",
    "    of the retain dataset\n",
    "    \n",
    "    Args:\n",
    "        net: nn.Module\n",
    "            degraded model\n",
    "        retain : torch.utils.data.DataLoader.\n",
    "            Dataset loader for access to the retain set. This is the subset\n",
    "            of the training set that we don't want to forget.\n",
    "        itter: int\n",
    "            Number of iterations to repair the model for\n",
    "    \n",
    "    Returns:\n",
    "        net: nn.Module\n",
    "            repaired model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    epochs = 2\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    net.train()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        print(f'epoch {_}')\n",
    "        for inputs, targets in tqdm(retain):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    net.eval()\n",
    "    return net\n",
    "    \n",
    "def degrade_model(net, forget, epoch):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "#     optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.009, weight_decay=5e-4)\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epoch)\n",
    "    net.train()\n",
    "    \n",
    "    for i in tqdm(range(epoch)):\n",
    "        inputs, targets = next(iter(forget))\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = -1 * criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#     scheduler.step()\n",
    "\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "    \n",
    "    \n",
    "def unlearner_v0(net, retain, forget, validation):\n",
    "    \"\"\"Unlearning through noise.\n",
    "\n",
    "    Args:\n",
    "        net : nn.Module.\n",
    "            pre-trained model to use as base of unlearning.\n",
    "        retain : torch.utils.data.DataLoader.\n",
    "            Dataset loader for access to the retain set. This is the subset\n",
    "            of the training set that we don't want to forget.\n",
    "        forget : torch.utils.data.DataLoader.\n",
    "            Dataset loader for access to the forget set. This is the subset\n",
    "            of the training set that we want to forget. This method doesn't\n",
    "            make use of the forget set.\n",
    "        validation : torch.utils.data.DataLoader.\n",
    "            Dataset loader for access to the validation set. This method doesn't\n",
    "            make use of the validation set.\n",
    "            \n",
    "        f_size : int\n",
    "            The size of the mini-batches used for the training of the noisy image\n",
    "        \n",
    "    Returns:\n",
    "          new_net : nn.Module\n",
    "              Updated model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train the noise layer on the forget dataset sample\n",
    "    inputs, targets = next(iter(forget))\n",
    "    \n",
    "    for j in range(3):\n",
    "        print(f'itteration {j}')\n",
    "        # Train the network using the noise layer\n",
    "        net = degrade_model(net.train(), forget, 5)\n",
    "\n",
    "        # Repair by training with sample of retain dataset sample\n",
    "        net = repair_model(net.train(), retain, 5)\n",
    "    \n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c72e69c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:24:13.460095Z",
     "iopub.status.busy": "2023-08-15T00:24:13.459682Z",
     "iopub.status.idle": "2023-08-15T00:24:13.468928Z",
     "shell.execute_reply": "2023-08-15T00:24:13.468108Z"
    },
    "papermill": {
     "duration": 0.104999,
     "end_time": "2023-08-15T00:24:13.471138",
     "exception": false,
     "start_time": "2023-08-15T00:24:13.366139",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "import copy\n",
    "model_copy = copy.deepcopy(in_model)\n",
    "\n",
    "v0_model = unlearner_v0(model_copy, retain_loader, forget_loader, test_loader)\n",
    "\n",
    "print(f\"Train set accuracy: {100.0 * accuracy(in_model, train_loader):0.1f}%\")\n",
    "print(f\"Test set accuracy: {100.0 * accuracy(in_model, test_loader):0.1f}%\")\n",
    "print(f\"Retrain set accuracy: {100.0 * accuracy(in_model, retain_loader):0.1f}%\")\n",
    "print(f\"Forget set accuracy: {100.0 * accuracy(in_model, forget_loader):0.1f}%\")\n",
    "\n",
    "print(f\"Train set accuracy: {100.0 * accuracy(v0_model, train_loader):0.1f}%\")\n",
    "print(f\"Test set accuracy: {100.0 * accuracy(v0_model, test_loader):0.1f}%\")\n",
    "print(f\"Retain set accuracy: {100.0 * accuracy(v0_model, retain_loader):0.1f}%\")\n",
    "print(f\"Forget set accuracy: {100.0 * accuracy(v0_model, forget_loader):0.1f}%\")\n",
    "\n",
    "# Additional retain results:\n",
    "# Train = 98.4\n",
    "# Test = 82.8\n",
    "# Retain = 99.9\n",
    "# Forget = 84.4\n",
    "\n",
    "# 95, 91, 79, 92, 79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae7c6cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:24:13.655692Z",
     "iopub.status.busy": "2023-08-15T00:24:13.655256Z",
     "iopub.status.idle": "2023-08-15T00:24:13.663611Z",
     "shell.execute_reply": "2023-08-15T00:24:13.662748Z"
    },
    "papermill": {
     "duration": 0.103863,
     "end_time": "2023-08-15T00:24:13.665836",
     "exception": false,
     "start_time": "2023-08-15T00:24:13.561973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "v0_test_losses = compute_losses(v0_model, test_loader)\n",
    "v0_forget_losses = compute_losses(v0_model, forget_loader)\n",
    "\n",
    "v0_samples_mia = np.concatenate((v0_test_losses, v0_forget_losses)).reshape((-1, 1))\n",
    "labels_mia = [0] * len(v0_test_losses) + [1] * len(v0_forget_losses)\n",
    "\n",
    "v0_mia_scores = simple_mia(v0_samples_mia, labels_mia)\n",
    "\n",
    "print(\n",
    "    f\"The MIA has an accuracy of {v0_mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd33f3d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:24:13.847592Z",
     "iopub.status.busy": "2023-08-15T00:24:13.847153Z",
     "iopub.status.idle": "2023-08-15T00:24:13.856297Z",
     "shell.execute_reply": "2023-08-15T00:24:13.855280Z"
    },
    "papermill": {
     "duration": 0.102837,
     "end_time": "2023-08-15T00:24:13.858586",
     "exception": false,
     "start_time": "2023-08-15T00:24:13.755749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.set_title(f\"Re-trained model.\\nAttack accuracy: {rt_mia_scores.mean():0.2f}\")\n",
    "ax1.hist(rt_test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "ax1.hist(rt_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "ax2.set_title(\n",
    "    f\"Attempt v0.1.\\nAttack accuracy: {v0_mia_scores.mean():0.2f}\"\n",
    ")\n",
    "ax2.hist(v0_test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "ax2.hist(v0_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "ax1.set_xlabel(\"Loss\")\n",
    "ax2.set_xlabel(\"Loss\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax1.set_xlim((0, np.max(v0_test_losses)))\n",
    "ax2.set_xlim((0, np.max(v0_test_losses)))\n",
    "for ax in (ax1, ax2):\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "ax1.legend(frameon=False, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a6b02a",
   "metadata": {
    "papermill": {
     "duration": 0.090555,
     "end_time": "2023-08-15T00:24:14.040814",
     "exception": false,
     "start_time": "2023-08-15T00:24:13.950259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Attempt v0.1\n",
    "More nuanced alternating between damaging and repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9366c4eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:24:14.224454Z",
     "iopub.status.busy": "2023-08-15T00:24:14.223378Z",
     "iopub.status.idle": "2023-08-15T00:24:14.238612Z",
     "shell.execute_reply": "2023-08-15T00:24:14.237486Z"
    },
    "papermill": {
     "duration": 0.109778,
     "end_time": "2023-08-15T00:24:14.241171",
     "exception": false,
     "start_time": "2023-08-15T00:24:14.131393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deg_rep_model(net, retain, forget, epoch, ratio):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        net: network\n",
    "        forget: forget dataloader\n",
    "        retain: retain dataloader\n",
    "        epoch: number of epochs\n",
    "        ratio: [int] epoch ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    deg_optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    deg_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(deg_optimizer, T_max=epoch)\n",
    "    rep_optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    rep_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(rep_optimizer, T_max=epoch)\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    for j in range(epoch):\n",
    "        print(f'Epoch: {j}')\n",
    "    \n",
    "        for deg_inputs, deg_targets in tqdm(forget):\n",
    "\n",
    "            # Degrade step:\n",
    "            deg_inputs, deg_targets = deg_inputs.to(DEVICE), deg_targets.to(DEVICE)\n",
    "            deg_optimizer.zero_grad()\n",
    "            deg_outputs = net(deg_inputs)\n",
    "            deg_loss = criterion(deg_outputs,deg_targets)\n",
    "            deg_loss = -1 * criterion(deg_outputs, deg_targets)\n",
    "            deg_loss.backward()\n",
    "            deg_optimizer.step()\n",
    "\n",
    "            # Repair step:\n",
    "            for i in range(ratio):\n",
    "                rep_inputs, rep_targets = next(iter(retain))\n",
    "                rep_inputs, rep_targets = rep_inputs.to(DEVICE), rep_targets.to(DEVICE)\n",
    "                rep_optimizer.zero_grad()\n",
    "                rep_outputs = net(rep_inputs)\n",
    "                rep_loss = criterion(rep_outputs, rep_targets)\n",
    "                rep_loss.backward()\n",
    "                rep_optimizer.step()\n",
    "        deg_scheduler.step()\n",
    "        rep_scheduler.step()\n",
    "    \n",
    "    net.eval()\n",
    "    return net\n",
    "    \n",
    "def unlearner_v01(net, retain, forget, epoch, ratio):\n",
    "    \"\"\"Unlearning through breaking followed by repair\n",
    "\n",
    "    Args:\n",
    "        net : nn.Module.\n",
    "            pre-trained model to use as base of unlearning.\n",
    "        retain : torch.utils.data.DataLoader.\n",
    "            Dataset loader for access to the retain set. This is the subset\n",
    "            of the training set that we don't want to forget.\n",
    "        forget : torch.utils.data.DataLoader.\n",
    "            Dataset loader for access to the forget set. This is the subset\n",
    "            of the training set that we want to forget. This method doesn't\n",
    "            make use of the forget set.\n",
    "        validation : torch.utils.data.DataLoader.\n",
    "            Dataset loader for access to the validation set. This method doesn't\n",
    "            make use of the validation set.\n",
    "            \n",
    "        f_size : int\n",
    "            The size of the mini-batches used for the training of the noisy image\n",
    "        \n",
    "    Returns:\n",
    "          new_net : nn.Module\n",
    "              Updated model\n",
    "    \"\"\"\n",
    "    \n",
    "    net = deg_rep_model(net, retain, forget, epoch, ratio)\n",
    "    \n",
    "    print(f\"Train set accuracy: {100.0 * accuracy(net, train_loader):0.1f}%\")\n",
    "    print(f\"Test set accuracy: {100.0 * accuracy(net, test_loader):0.1f}%\")\n",
    "    print(f\"Retain set accuracy: {100.0 * accuracy(net, retain_loader):0.1f}%\")\n",
    "    print(f\"Forget set accuracy: {100.0 * accuracy(net, forget_loader):0.1f}%\")\n",
    "    print(f'Compared to: \\n Train = 95.9% \\n Test = 85.0% \\n Retain = 97.1% \\n Forget = 85.6%')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8a26031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T00:24:14.429946Z",
     "iopub.status.busy": "2023-08-15T00:24:14.428661Z",
     "iopub.status.idle": "2023-08-15T09:07:33.832037Z",
     "shell.execute_reply": "2023-08-15T09:07:33.830605Z"
    },
    "papermill": {
     "duration": 31399.500801,
     "end_time": "2023-08-15T09:07:33.834349",
     "exception": false,
     "start_time": "2023-08-15T00:24:14.333548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings : 4, 10\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [07:37<00:00, 11.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [07:16<00:00, 10.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [07:33<00:00, 11.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [07:30<00:00, 11.27s/it]\n",
      "100%|██████████| 391/391 [01:08<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 93.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 80.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [01:01<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain set accuracy: 95.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget set accuracy: 72.3%\n",
      "Compared to: \n",
      " Train = 95.9% \n",
      " Test = 85.0% \n",
      " Retain = 97.1% \n",
      " Forget = 85.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.53it/s]\n",
      "100%|██████████| 40/40 [00:07<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MIA has an accuracy of 0.550 on forgotten vs unseen images\n",
      "Settings : 4, 15\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:56<00:00, 16.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:49<00:00, 16.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:31<00:00, 15.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:50<00:00, 16.26s/it]\n",
      "100%|██████████| 391/391 [01:06<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 95.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 80.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [01:00<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain set accuracy: 97.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget set accuracy: 72.6%\n",
      "Compared to: \n",
      " Train = 95.9% \n",
      " Test = 85.0% \n",
      " Retain = 97.1% \n",
      " Forget = 85.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.64it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MIA has an accuracy of 0.543 on forgotten vs unseen images\n",
      "Settings : 4, 20\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [14:12<00:00, 21.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [14:25<00:00, 21.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [14:16<00:00, 21.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [14:02<00:00, 21.06s/it]\n",
      "100%|██████████| 391/391 [01:05<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 96.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 81.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [01:02<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain set accuracy: 99.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget set accuracy: 72.7%\n",
      "Compared to: \n",
      " Train = 95.9% \n",
      " Test = 85.0% \n",
      " Retain = 97.1% \n",
      " Forget = 85.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.24it/s]\n",
      "100%|██████████| 40/40 [00:07<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MIA has an accuracy of 0.544 on forgotten vs unseen images\n",
      "Settings : 5, 10\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [07:30<00:00, 11.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [07:02<00:00, 10.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [07:16<00:00, 10.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [07:16<00:00, 10.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [07:39<00:00, 11.48s/it]\n",
      "100%|██████████| 391/391 [01:07<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 94.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 80.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [01:01<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain set accuracy: 96.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget set accuracy: 68.8%\n",
      "Compared to: \n",
      " Train = 95.9% \n",
      " Test = 85.0% \n",
      " Retain = 97.1% \n",
      " Forget = 85.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.16it/s]\n",
      "100%|██████████| 40/40 [00:07<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MIA has an accuracy of 0.563 on forgotten vs unseen images\n",
      "Settings : 5, 15\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:46<00:00, 16.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:22<00:00, 15.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:52<00:00, 16.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:40<00:00, 16.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [04:33<06:10, 16.09s/it]Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/opt/conda/lib/python3.10/shutil.py\", line 731, in rmtree\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"/opt/conda/lib/python3.10/shutil.py\", line 729, in rmtree\n",
      "    os.rmdir(path)\n",
      "OSError: [Errno 39] Directory not empty: '/tmp/pymp-twbfrj_w'\n",
      " 90%|█████████ | 36/40 [09:50<01:09, 17.32s/it]Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/opt/conda/lib/python3.10/shutil.py\", line 731, in rmtree\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"/opt/conda/lib/python3.10/shutil.py\", line 729, in rmtree\n",
      "    os.rmdir(path)\n",
      "OSError: [Errno 39] Directory not empty: '/tmp/pymp-eksmhsyi'\n",
      "100%|██████████| 40/40 [10:59<00:00, 16.48s/it]\n",
      "100%|██████████| 391/391 [01:11<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 96.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 81.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [01:08<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain set accuracy: 99.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget set accuracy: 69.8%\n",
      "Compared to: \n",
      " Train = 95.9% \n",
      " Test = 85.0% \n",
      " Retain = 97.1% \n",
      " Forget = 85.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.36it/s]\n",
      "100%|██████████| 40/40 [00:07<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MIA has an accuracy of 0.562 on forgotten vs unseen images\n",
      "Settings : 5, 20\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [14:37<00:00, 21.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [14:33<00:00, 21.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [16:19<00:00, 24.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [15:04<00:00, 22.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [14:11<00:00, 21.30s/it]\n",
      "100%|██████████| 391/391 [01:08<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 96.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 81.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [01:02<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain set accuracy: 99.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget set accuracy: 71.1%\n",
      "Compared to: \n",
      " Train = 95.9% \n",
      " Test = 85.0% \n",
      " Retain = 97.1% \n",
      " Forget = 85.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.51it/s]\n",
      "100%|██████████| 40/40 [00:07<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MIA has an accuracy of 0.554 on forgotten vs unseen images\n",
      "Settings : 6, 10\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [01:54<05:38, 11.30s/it]Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/opt/conda/lib/python3.10/shutil.py\", line 731, in rmtree\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"/opt/conda/lib/python3.10/shutil.py\", line 729, in rmtree\n",
      "    os.rmdir(path)\n",
      "OSError: [Errno 39] Directory not empty: '/tmp/pymp-sq59oleo'\n",
      "100%|██████████| 40/40 [07:40<00:00, 11.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [04:39<02:56, 11.02s/it]Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/opt/conda/lib/python3.10/shutil.py\", line 731, in rmtree\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"/opt/conda/lib/python3.10/shutil.py\", line 729, in rmtree\n",
      "    os.rmdir(path)\n",
      "OSError: [Errno 39] Directory not empty: '/tmp/pymp-_wnx2axc'\n",
      "100%|██████████| 40/40 [07:25<00:00, 11.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [06:46<00:00, 10.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [07:01<00:00, 10.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [08:14<00:00, 12.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [06:52<00:00, 10.30s/it]\n",
      "100%|██████████| 391/391 [01:04<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 95.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 80.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:57<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain set accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget set accuracy: 69.1%\n",
      "Compared to: \n",
      " Train = 95.9% \n",
      " Test = 85.0% \n",
      " Retain = 97.1% \n",
      " Forget = 85.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.19it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MIA has an accuracy of 0.562 on forgotten vs unseen images\n",
      "Settings : 6, 15\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:47<00:00, 16.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:51<00:00, 16.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:20<00:00, 15.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:56<00:00, 16.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [11:05<00:00, 16.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [11:04<00:00, 16.61s/it]\n",
      "100%|██████████| 391/391 [01:06<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 96.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 81.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:57<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain set accuracy: 99.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget set accuracy: 68.0%\n",
      "Compared to: \n",
      " Train = 95.9% \n",
      " Test = 85.0% \n",
      " Retain = 97.1% \n",
      " Forget = 85.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  5.81it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MIA has an accuracy of 0.576 on forgotten vs unseen images\n",
      "Settings : 6, 20\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [14:34<00:00, 21.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [13:46<00:00, 20.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [14:16<00:00, 21.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [14:20<00:00, 21.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [18:18<00:00, 27.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [20:01<00:00, 30.04s/it]\n",
      "100%|██████████| 391/391 [01:07<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 96.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 80.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [01:00<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain set accuracy: 99.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget set accuracy: 68.3%\n",
      "Compared to: \n",
      " Train = 95.9% \n",
      " Test = 85.0% \n",
      " Retain = 97.1% \n",
      " Forget = 85.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.67it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MIA has an accuracy of 0.561 on forgotten vs unseen images\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from itertools import permutations\n",
    "\n",
    "# model_copy = copy.deepcopy(in_model)\n",
    "\n",
    "ratio_list = [10, 15, 20]\n",
    "epoch_list = [4, 5, 6]\n",
    "\n",
    "for i in epoch_list:\n",
    "    for j in ratio_list:\n",
    "        print(f'Settings : {i}, {j}')\n",
    "        model_copy = copy.deepcopy(in_model)\n",
    "        v01_model = unlearner_v01(model_copy, retain_loader, forget_loader, i, j)\n",
    "\n",
    "        v01_test_losses = compute_losses(v01_model, test_loader)\n",
    "        v01_forget_losses = compute_losses(v01_model, forget_loader)\n",
    "\n",
    "        v01_samples_mia = np.concatenate((v01_test_losses, v01_forget_losses)).reshape((-1, 1))\n",
    "        labels_mia = [0] * len(v01_test_losses) + [1] * len(v01_forget_losses)\n",
    "\n",
    "        v01_mia_scores = simple_mia(v01_samples_mia, labels_mia)\n",
    "\n",
    "        print(\n",
    "            f\"The MIA has an accuracy of {v01_mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "514c1c81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T09:07:35.875862Z",
     "iopub.status.busy": "2023-08-15T09:07:35.874979Z",
     "iopub.status.idle": "2023-08-15T09:07:35.883427Z",
     "shell.execute_reply": "2023-08-15T09:07:35.882646Z"
    },
    "papermill": {
     "duration": 1.019855,
     "end_time": "2023-08-15T09:07:35.885663",
     "exception": false,
     "start_time": "2023-08-15T09:07:34.865808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "v01_test_losses = compute_losses(v01_model, test_loader)\n",
    "v01_forget_losses = compute_losses(v01_model, forget_loader)\n",
    "\n",
    "v01_samples_mia = np.concatenate((v01_test_losses, v01_forget_losses)).reshape((-1, 1))\n",
    "labels_mia = [0] * len(v01_test_losses) + [1] * len(v01_forget_losses)\n",
    "\n",
    "v01_mia_scores = simple_mia(v01_samples_mia, labels_mia)\n",
    "\n",
    "print(\n",
    "    f\"The MIA has an accuracy of {v01_mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6553",
   "metadata": {
    "papermill": {
     "duration": 1.018334,
     "end_time": "2023-08-15T09:07:37.907172",
     "exception": false,
     "start_time": "2023-08-15T09:07:36.888838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Attempt v1.0\n",
    "Using the [Fast Yet Effective Machine Unlearning](https://arxiv.org/pdf/2111.08947.pdf) with [code](https://github.com/vikram2000b/Fast-Machine-Unlearning/blob/main/Machine%20Unlearning.ipynb) as inspiration. The idea behind this model is to selectively train a layer to generate a noisy image to be added to each of the target forget images. A few questions still to be answered:\n",
    "- Can the noisy image be generated for multiple images at the same time? As in, pairs of 2/5/10/... images are used to train the noise. That would significantly reduce the amount of time it takes to forget.\n",
    "- Is it better to generate this noisy image for groups based on classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c114ca1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T09:07:40.068649Z",
     "iopub.status.busy": "2023-08-15T09:07:40.068254Z",
     "iopub.status.idle": "2023-08-15T09:07:40.076002Z",
     "shell.execute_reply": "2023-08-15T09:07:40.075184Z"
    },
    "papermill": {
     "duration": 1.025685,
     "end_time": "2023-08-15T09:07:40.078292",
     "exception": false,
     "start_time": "2023-08-15T09:07:39.052607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Create Noise Class\n",
    "class Noise(nn.Module):\n",
    "    def __init__(self, *dim):\n",
    "        super().__init__()\n",
    "        self.noise = torch.nn.Parameter(torch.randn(*dim), requires_grad = True)\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee268f9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T09:07:42.086939Z",
     "iopub.status.busy": "2023-08-15T09:07:42.086569Z",
     "iopub.status.idle": "2023-08-15T09:07:42.098652Z",
     "shell.execute_reply": "2023-08-15T09:07:42.097650Z"
    },
    "papermill": {
     "duration": 1.018247,
     "end_time": "2023-08-15T09:07:42.100782",
     "exception": false,
     "start_time": "2023-08-15T09:07:41.082535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "def train_noise(net, noise, inputs, targets):\n",
    "    \"\"\"Train the noisy layer\n",
    "    \n",
    "    Args:\n",
    "        net : nn.Module\n",
    "            pre-trained model with weights locked\n",
    "        noise : nn.Module\n",
    "            noisy layer\n",
    "        inputs : ???\n",
    "            subset of images from the forget\n",
    "        targets : ???\n",
    "            vector of labels\n",
    "    \n",
    "    Returns:\n",
    "        noise : np.array\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "#     print(noise.get_parameter('noise'))\n",
    "    \n",
    "    epochs = 50\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(noise.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    print('Training Noise:')\n",
    "    for _ in tqdm(range(epochs)):\n",
    "#         print(f'epoch {_}')\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs + noise())\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss = loss * -1\n",
    "#         print(loss.detach())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    return noise\n",
    "    \n",
    "\n",
    "def degrade_model(net, noise, inputs, targets):\n",
    "    \"\"\"Takes the given model and noise and uses that to\n",
    "    degrade the model\n",
    "    \n",
    "    Args:\n",
    "        net : nn.Module\n",
    "            pre-trained model to degrade\n",
    "        noise : nn.Module\n",
    "            generated noise image to add to inputs\n",
    "        target : ???\n",
    "        \n",
    "    Returns:\n",
    "        net : nn.Module\n",
    "            Degraded network\n",
    "    \n",
    "    \"\"\"\n",
    "    epochs = 3\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    net.train()\n",
    "    \n",
    "    print('Degrading Model:')\n",
    "    for _ in tqdm(range(epochs)):\n",
    "#         print(f'epoch {_}')\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs + noise())\n",
    "        loss = criterion(outputs, targets)\n",
    "#         print(loss.detach())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    return net\n",
    "    \n",
    "def repair_model(net, retain, itter):\n",
    "    \"\"\"Repairs the given model using a couple itterations\n",
    "    of the retain dataset\n",
    "    \n",
    "    Args:\n",
    "        net: nn.Module\n",
    "            degraded model\n",
    "        retain : torch.utils.data.DataLoader.\n",
    "            Dataset loader for access to the retain set. This is the subset\n",
    "            of the training set that we don't want to forget.\n",
    "        itter: int\n",
    "            Number of iterations to repair the model for\n",
    "    \n",
    "    Returns:\n",
    "        net: nn.Module\n",
    "            repaired model\n",
    "    \n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=itter)\n",
    "    net.train()\n",
    "\n",
    "    print('Repairing Model:')\n",
    "    for _ in tqdm(range(itter)):\n",
    "#         print(f'itter {_}')\n",
    "#         for inputs, targets in tqdm(retain):\n",
    "        inputs, targets =  next(iter(retain))\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    net.eval()\n",
    "    return net\n",
    "    \n",
    "    \n",
    "    \n",
    "def unlearner_v10(net, retain, forget, validation, f_size):\n",
    "    \"\"\"Unlearning through noise.\n",
    "\n",
    "    Args:\n",
    "        net : nn.Module.\n",
    "            pre-trained model to use as base of unlearning.\n",
    "        retain : torch.utils.data.DataLoader.\n",
    "            Dataset loader for access to the retain set. This is the subset\n",
    "            of the training set that we don't want to forget.\n",
    "        forget : torch.utils.data.DataLoader.\n",
    "            Dataset loader for access to the forget set. This is the subset\n",
    "            of the training set that we want to forget. This method doesn't\n",
    "            make use of the forget set.\n",
    "        validation : torch.utils.data.DataLoader.\n",
    "            Dataset loader for access to the validation set. This method doesn't\n",
    "            make use of the validation set.\n",
    "            \n",
    "        f_size : int\n",
    "            The size of the mini-batches used for the training of the noisy image\n",
    "        \n",
    "    Returns:\n",
    "          new_net : nn.Module\n",
    "              Updated model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train the noise layer on the forget dataset sample\n",
    "    inputs, targets = next(iter(forget))\n",
    "    \n",
    "    noise = Noise(inputs.shape).to(DEVICE)\n",
    "    \n",
    "    noise = train_noise(net.eval(), noise.train(), inputs, targets)\n",
    "    \n",
    "    # Train the network using the noise layer\n",
    "    net = degrade_model(net.train(), noise.eval(), inputs, targets)\n",
    "    \n",
    "    # Repair by training with sample of retain dataset sample\n",
    "    net = repair_model(net.train(), retain, 300)\n",
    "    \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f015333c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T09:07:44.303585Z",
     "iopub.status.busy": "2023-08-15T09:07:44.303163Z",
     "iopub.status.idle": "2023-08-15T09:07:44.311617Z",
     "shell.execute_reply": "2023-08-15T09:07:44.310468Z"
    },
    "papermill": {
     "duration": 1.053658,
     "end_time": "2023-08-15T09:07:44.313714",
     "exception": false,
     "start_time": "2023-08-15T09:07:43.260056",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "import copy\n",
    "model_copy = copy.deepcopy(ft_model)\n",
    "\n",
    "v10_model = unlearner_v10(ft_model, retain_loader, forget_loader, test_loader, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5dc529b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T09:07:46.360401Z",
     "iopub.status.busy": "2023-08-15T09:07:46.359706Z",
     "iopub.status.idle": "2023-08-15T09:07:46.367120Z",
     "shell.execute_reply": "2023-08-15T09:07:46.366255Z"
    },
    "papermill": {
     "duration": 1.029994,
     "end_time": "2023-08-15T09:07:46.369472",
     "exception": false,
     "start_time": "2023-08-15T09:07:45.339478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "print(f\"Train set accuracy: {100.0 * accuracy(v10_model, train_loader):0.1f}%\")\n",
    "print(f\"Test set accuracy: {100.0 * accuracy(v10_model, test_loader):0.1f}%\")\n",
    "print(f\"Retain set accuracy: {100.0 * accuracy(v10_model, retain_loader):0.1f}%\")\n",
    "print(f\"Forget set accuracy: {100.0 * accuracy(v10_model, forget_loader):0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfef13d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T09:07:48.380468Z",
     "iopub.status.busy": "2023-08-15T09:07:48.379717Z",
     "iopub.status.idle": "2023-08-15T09:07:48.387611Z",
     "shell.execute_reply": "2023-08-15T09:07:48.386568Z"
    },
    "papermill": {
     "duration": 1.022832,
     "end_time": "2023-08-15T09:07:48.390114",
     "exception": false,
     "start_time": "2023-08-15T09:07:47.367282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "v10_test_losses = compute_losses(v10_model, test_loader)\n",
    "v10_forget_losses = compute_losses(v10_model, forget_loader)\n",
    "\n",
    "v10_samples_mia = np.concatenate((v10_test_losses, v10_forget_losses)).reshape((-1, 1))\n",
    "labels_mia = [0] * len(v10_test_losses) + [1] * len(v10_forget_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d1cf888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T09:07:50.529093Z",
     "iopub.status.busy": "2023-08-15T09:07:50.528721Z",
     "iopub.status.idle": "2023-08-15T09:07:50.536600Z",
     "shell.execute_reply": "2023-08-15T09:07:50.535844Z"
    },
    "papermill": {
     "duration": 1.012386,
     "end_time": "2023-08-15T09:07:50.538728",
     "exception": false,
     "start_time": "2023-08-15T09:07:49.526342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "v10_mia_scores = simple_mia(v10_samples_mia, labels_mia)\n",
    "\n",
    "print(\n",
    "    f\"The MIA has an accuracy of {v10_mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0edb76a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T09:07:52.557046Z",
     "iopub.status.busy": "2023-08-15T09:07:52.555924Z",
     "iopub.status.idle": "2023-08-15T09:07:52.564870Z",
     "shell.execute_reply": "2023-08-15T09:07:52.564083Z"
    },
    "papermill": {
     "duration": 1.023797,
     "end_time": "2023-08-15T09:07:52.567319",
     "exception": false,
     "start_time": "2023-08-15T09:07:51.543522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.set_title(f\"Re-trained model.\\nAttack accuracy: {rt_mia_scores.mean():0.2f}\")\n",
    "ax1.hist(rt_test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "ax1.hist(rt_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "ax2.set_title(\n",
    "    f\"Attempt v1.0 \\nAttack accuracy: {v10_mia_scores.mean():0.2f}\"\n",
    ")\n",
    "ax2.hist(v10_test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "ax2.hist(v10_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "\n",
    "ax1.set_xlabel(\"Loss\")\n",
    "ax2.set_xlabel(\"Loss\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax1.set_xlim((0, np.max(v10_test_losses)))\n",
    "ax2.set_xlim((0, np.max(v10_test_losses)))\n",
    "for ax in (ax1, ax2):\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "ax1.legend(frameon=False, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56ac43",
   "metadata": {
    "papermill": {
     "duration": 1.022025,
     "end_time": "2023-08-15T09:07:54.723390",
     "exception": false,
     "start_time": "2023-08-15T09:07:53.701365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31601.617539,
   "end_time": "2023-08-15T09:07:57.332824",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-15T00:21:15.715285",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
