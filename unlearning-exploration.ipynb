{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NeurIPS 2023 Machine Unlearning Challenge\nHeavily based off of the code presented here: https://github.com/unlearning-challenge/starting-kit/blob/main/unlearning-CIFAR10.ipynb","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-05T20:24:18.056311Z","iopub.execute_input":"2023-08-05T20:24:18.056789Z","iopub.status.idle":"2023-08-05T20:24:18.071466Z","shell.execute_reply.started":"2023-08-05T20:24:18.056755Z","shell.execute_reply":"2023-08-05T20:24:18.070217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model, model_selection\n\nfrom tqdm import tqdm\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nfrom torchvision.models import resnet18\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Running on device:\", DEVICE.upper())\n\n# manual random seed is used for dataset partitioning\n# to ensure reproducible results across runs\nRNG = torch.Generator().manual_seed(42)\n\n# download and pre-process CIFAR10\nnormalize = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ]\n)\n\ntrain_set = torchvision.datasets.CIFAR10(\n    root=\"./data\", train=True, download=True, transform=normalize\n)\ntrain_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n\n# we split held out data into test and validation set\nheld_out = torchvision.datasets.CIFAR10(\n    root=\"./data\", train=False, download=True, transform=normalize\n)\ntest_set, val_set = torch.utils.data.random_split(held_out, [0.5, 0.5], generator=RNG)\ntest_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\nval_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n\n# for the unlearning algorithm we'll also need a split of the train set into\n# forget_set and a retain_set\nforget_set, retain_set = torch.utils.data.random_split(train_set, [0.1, 0.9], generator=RNG)\nforget_loader = torch.utils.data.DataLoader(\n    forget_set, batch_size=128, shuffle=True, num_workers=2\n)\nretain_loader = torch.utils.data.DataLoader(\n    retain_set, batch_size=128, shuffle=True, num_workers=2, generator=RNG\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-05T20:24:20.291621Z","iopub.execute_input":"2023-08-05T20:24:20.292529Z","iopub.status.idle":"2023-08-05T20:24:35.811978Z","shell.execute_reply.started":"2023-08-05T20:24:20.292483Z","shell.execute_reply":"2023-08-05T20:24:35.810621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we can't download the aws weights, I've manually uploaded them to this Kaggle notebook as `unlearning-cifar10-resnet18-weights`","metadata":{}},{"cell_type":"code","source":"# # download pre-trained weights\n# local_path = \"weights_resnet18_cifar10.pth\"\n# if not os.path.exists(local_path):\n#     response = requests.get(\n#         'https://unlearning-challenge.s3.eu_west-1.amazonaws.com/weights_resnte18_cifar10.pth'\n#     )\n#     open(local_path, 'wb').write(response.content)\n\n# weights_pretrained = torch.load(local_path, map_location=DEVICE)\n\n# # Load model with pre-trained weights\n# model = resnet18(weights=None, num_classes=10)\n# model.load_state_dict(weights_pretrained)\n# model.to(DEVICE)\n# model.eval()\n\nweights_pretrained = torch.load('/kaggle/input/unlearning-cifar10-resnet18-weights/unlearning-cifar10-resnet18-weights/weights_resnet18_cifar10.pth', map_location=DEVICE)\nmodel = resnet18(weights=None, num_classes=10)\nmodel.load_state_dict(weights_pretrained)\nmodel.to(DEVICE)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T20:24:35.815174Z","iopub.execute_input":"2023-08-05T20:24:35.816491Z","iopub.status.idle":"2023-08-05T20:24:36.537965Z","shell.execute_reply.started":"2023-08-05T20:24:35.816439Z","shell.execute_reply":"2023-08-05T20:24:36.536169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Look at sample images","metadata":{}},{"cell_type":"code","source":"# a temporary data loader without normalization, just to show the images\ntmp_dl = DataLoader(\n    torchvision.datasets.CIFAR10(\n        root=\"./data\", train=True, download=True, transform=transforms.ToTensor()\n    ),\n    batch_size=16 * 5,\n    shuffle=False,\n)\nimages, labels = next(iter(tmp_dl))\n\nfig, ax = plt.subplots(figsize=(12, 6))\nplt.title(\"Sample images from CIFAR10 dataset\")\nax.set_xticks([])\nax.set_yticks([])\nax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T18:23:13.537688Z","iopub.execute_input":"2023-08-05T18:23:13.538135Z","iopub.status.idle":"2023-08-05T18:23:15.141927Z","shell.execute_reply.started":"2023-08-05T18:23:13.538093Z","shell.execute_reply":"2023-08-05T18:23:15.140907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Accuracy Equation","metadata":{}},{"cell_type":"code","source":"def accuracy(net, loader):\n    \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n    correct = 0\n    total = 0\n    for inputs, targets in tqdm(loader):\n        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n        outputs = net(inputs)\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n    return correct / total\n\n\nprint(f\"Train set accuracy: {100.0 * accuracy(model, train_loader):0.1f}%\")\nprint(f\"Test set accuracy: {100.0 * accuracy(model, test_loader):0.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-08-05T20:24:36.541332Z","iopub.execute_input":"2023-08-05T20:24:36.541872Z","iopub.status.idle":"2023-08-05T20:25:58.629212Z","shell.execute_reply.started":"2023-08-05T20:24:36.541815Z","shell.execute_reply":"2023-08-05T20:25:58.627271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unlearning Algorithm\nIn this section we develop the unlearning algorithm.\n\nIn the previous section we created a split of the original training set into a retain set and a forget set. Typically, the retain set is much larger than the forget set. Here, we produce a split that is 10% forget set, and 90% retain set.\n\nThe goal of an unlearning algorithm is to produce a model that approximates as much as possible the model trained solely on the retain set.\n\nBelow is a simple unlearning algorithm provided for illustration purposes. We call this algorithm unlearning by fine-tuning. It starts from the pre-trained model and optimizes it for a few epochs on the retain set. This is a very simple unlearning algorithm, but it is not very computationally efficient, and we don't expect it to work very well for most metrics.\n\nTo make a new entry in the competition, participants will submit an unlearning function with the same API as the one below. Note that the unlearning function takes as input a pre-trained model, a retain set, a forget set and an evaluation set (even though the fine-tuning algorithm below only uses the retain set and ignores the other datasets).","metadata":{}},{"cell_type":"code","source":"def unlearning(net, retain, forget, validation):\n    \"\"\"Unlearning by fine-tuning.\n\n    Fine-tuning is a very simple algorithm that trains using only\n    the retain set.\n\n    Args:\n      net : nn.Module.\n        pre-trained model to use as base of unlearning.\n      retain : torch.utils.data.DataLoader.\n        Dataset loader for access to the retain set. This is the subset\n        of the training set that we don't want to forget.\n      forget : torch.utils.data.DataLoader.\n        Dataset loader for access to the forget set. This is the subset\n        of the training set that we want to forget. This method doesn't\n        make use of the forget set.\n      validation : torch.utils.data.DataLoader.\n        Dataset loader for access to the validation set. This method doesn't\n        make use of the validation set.\n    Returns:\n      net : updated model\n    \"\"\"\n    epochs = 5\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    net.train()\n\n    for _ in range(epochs):\n        print(f'epoch {_}')\n        for inputs, targets in tqdm(retain):\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n            optimizer.zero_grad()\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n\n    net.eval()\n    return net","metadata":{"execution":{"iopub.status.busy":"2023-08-05T20:28:04.194505Z","iopub.execute_input":"2023-08-05T20:28:04.195269Z","iopub.status.idle":"2023-08-05T20:28:04.205235Z","shell.execute_reply.started":"2023-08-05T20:28:04.195229Z","shell.execute_reply":"2023-08-05T20:28:04.204133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ft_model = resnet18(weights=None, num_classes=10)\nft_model.load_state_dict(weights_pretrained)\nft_model.to(DEVICE)\n\n# Execute the unlearing routine. This might take a few minutes.\n# If run on colab, be sure to be running it on  an instance with GPUs\nft_model = unlearning(ft_model, retain_loader, forget_loader, test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T20:28:05.043433Z","iopub.execute_input":"2023-08-05T20:28:05.044448Z","iopub.status.idle":"2023-08-05T20:56:06.115548Z","shell.execute_reply.started":"2023-08-05T20:28:05.044386Z","shell.execute_reply":"2023-08-05T20:56:06.113230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Retain set accuracy: {100.0 * accuracy(ft_model, retain_loader):0.1f}%\")\nprint(f\"Test set accuracy: {100.0 * accuracy(ft_model, test_loader):0.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-08-05T20:56:06.118530Z","iopub.execute_input":"2023-08-05T20:56:06.119232Z","iopub.status.idle":"2023-08-05T20:57:20.405868Z","shell.execute_reply.started":"2023-08-05T20:56:06.119167Z","shell.execute_reply":"2023-08-05T20:57:20.404295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Re-trained model","metadata":{}},{"cell_type":"code","source":"weights_pretrained = torch.load('/kaggle/input/unlearning-cifar10-resnet18-weights/unlearning-cifar10-resnet18-weights/retrain_weights_resnet18_cifar10.pth', map_location=DEVICE)\n\n# load model with pre-trained weights\nrt_model = resnet18(weights=None, num_classes=10)\nrt_model.load_state_dict(weights_pretrained)\nrt_model.to(DEVICE)\nrt_model.eval()\n\n# print its accuracy on retain and forget set\nprint(f\"Retain set accuracy: {100.0 * accuracy(rt_model, retain_loader):0.1f}%\")\nprint(f\"Forget set accuracy: {100.0 * accuracy(rt_model, forget_loader):0.1f}%\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"def compute_losses(net, loader):\n    \"\"\"Auxiliary function to compute per-sample losses\"\"\"\n\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    all_losses = []\n\n    for inputs, targets in tqdm(loader):\n        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n\n        logits = net(inputs)\n        losses = criterion(logits, targets).numpy(force=True)\n        for l in losses:\n            all_losses.append(l)\n\n    return np.array(all_losses)\n\n\ntrain_losses = compute_losses(model, train_loader)\ntest_losses = compute_losses(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:05:03.772522Z","iopub.execute_input":"2023-08-05T21:05:03.773018Z","iopub.status.idle":"2023-08-05T21:06:24.073806Z","shell.execute_reply.started":"2023-08-05T21:05:03.772977Z","shell.execute_reply":"2023-08-05T21:06:24.072253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot losses on train and test set\nplt.title(\"Losses on train and test set (pre-trained model)\")\nplt.hist(test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\nplt.hist(train_losses, density=True, alpha=0.5, bins=50, label=\"Train set\")\nplt.xlabel(\"Loss\", fontsize=14)\nplt.ylabel(\"Frequency\", fontsize=14)\nplt.xlim((0, np.max(test_losses)))\nplt.yscale(\"log\")\nplt.legend(frameon=False, fontsize=14)\nax = plt.gca()\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:06:24.076841Z","iopub.execute_input":"2023-08-05T21:06:24.078337Z","iopub.status.idle":"2023-08-05T21:06:25.338727Z","shell.execute_reply.started":"2023-08-05T21:06:24.078272Z","shell.execute_reply":"2023-08-05T21:06:25.336985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def simple_mia(sample_loss, members, n_splits=10, random_state=0):\n    \"\"\"Computes cross-validation score of a membership inference attack.\n\n    Args:\n      sample_loss : array_like of shape (n,).\n        objective function evaluated on n samples.\n      members : array_like of shape (n,),\n        whether a sample was used for training.\n      n_splits: int\n        number of splits to use in the cross-validation.\n    Returns:\n      scores : array_like of size (n_splits,)\n    \"\"\"\n\n    unique_members = np.unique(members)\n    if not np.all(unique_members == np.array([0, 1])):\n        raise ValueError(\"members should only have 0 and 1s\")\n\n    attack_model = linear_model.LogisticRegression()\n    cv = model_selection.StratifiedShuffleSplit(\n        n_splits=n_splits, random_state=random_state\n    )\n    return model_selection.cross_val_score(\n        attack_model, sample_loss, members, cv=cv, scoring=\"accuracy\"\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rt_test_losses = compute_losses(rt_model, test_loader)\nrt_forget_losses = compute_losses(rt_model, forget_loader)\n\nrt_samples_mia = np.concatenate((rt_test_losses, rt_forget_losses)).reshape((-1, 1))\nlabels_mia = [0] * len(rt_test_losses) + [1] * len(rt_forget_losses)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attempts:\n- Fast Yet Effective Machine Unlearning (https://arxiv.org/pdf/2111.08947.pdf)\nhttps://github.com/vikram2000b/Fast-Machine-Unlearning/blob/main/Machine%20Unlearning.ipynb","metadata":{}},{"cell_type":"code","source":"# Create Noise Class\nclass Noise(nn.Module):\n    def __init__(self, *dim):\n        super().__init__()\n        self.noise = torch.nn.Parameter(torch.randn(*dim), required_grad = True)\n    \n    def forward(self):\n        return self.noise","metadata":{},"execution_count":null,"outputs":[]}]}